{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7Nqxf6kaWcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb7eda9-d5c3-4f06-fa25-1888e121e3df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: dataset/train-processed-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 code/preprocess.py dataset/train-processed.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/preprocess.py dataset/test-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FIEolyFw-s_",
        "outputId": "a1a55387-0755-4765-d821-478b995fd71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"code/preprocess.py\", line 107, in <module>\n",
            "    preprocess_csv(csv_file_name, processed_file_name, test_file=False)\n",
            "  File \"code/preprocess.py\", line 81, in preprocess_csv\n",
            "    positive = int(line[:line.find(',')])\n",
            "ValueError: invalid literal for int() with base 10: 'is so sad for my apl friend'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py dataset/train-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqzlcDCrwG8q",
        "outputId": "bc1a58a2-e229-48a0-ca52-41598810f977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/train-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/train-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 0, Avg: 0.0000, Max: 0\n",
            "URLs => Total: 0, Avg: 0.0000, Max: 0\n",
            "Emojis => Total: 0, Positive: 0, Negative: 0, Avg: 0.0000, Max: 0\n",
            "Words => Total: 1283269, Unique: 50361, Avg: 12.8327, Max: 41, Min: 0\n",
            "Bigrams => Total: 1183437, Unique: 392543, Avg: 11.8344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py dataset/test-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-aLB7YswWA2",
        "outputId": "21928bbb-390a-40d4-a767-db3c1fbe17e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/test-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/test-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 0, Positive: 0, Negative: 0\n",
            "Traceback (most recent call last):\n",
            "  File \"code/stats.py\", line 106, in <module>\n",
            "    print 'User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions)\n",
            "ZeroDivisionError: float division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-irfwaZ2wiQ3",
        "outputId": "9286acc7-cf59-408a-fc99-636509725cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EUc9dyKwnqP",
        "outputId": "9fb5018e-8d25-4850-a740-80e96e73b931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7779/10000 = 77.7900 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/logistic.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqwlPQiVwtGp",
        "outputId": "3b0ea802-93f4-4d92-9dfc-dc6baa93104f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-24 15:45:49.211666: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Iteration 10/180, loss:0.6853, acc:0.64002022-01-24 15:45:52.795796: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 22/180, loss:0.6797, acc:0.62402022-01-24 15:45:54.251754: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 30/180, loss:0.6671, acc:0.67402022-01-24 15:45:55.206112: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 72/180, loss:0.6452, acc:0.67802022-01-24 15:46:00.218508: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5960, acc:0.73202022-01-24 15:46:14.631011: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 1, val_acc:0.7362\n",
            "Accuracy improved from 0.0000 to 0.7362, saving model\n",
            "Iteration 4/180, loss:0.5910, acc:0.73402022-01-24 15:46:16.113052: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 34/180, loss:0.5918, acc:0.72602022-01-24 15:46:19.762007: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 61/180, loss:0.5688, acc:0.73802022-01-24 15:46:23.041500: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 173/180, loss:0.5550, acc:0.75602022-01-24 15:46:36.525584: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5617, acc:0.7340\n",
            "Epoch: 2, val_acc:0.7535\n",
            "Accuracy improved from 0.7362 to 0.7535, saving model\n",
            "Iteration 97/180, loss:0.5373, acc:0.74402022-01-24 15:46:51.349631: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5291, acc:0.7500\n",
            "Epoch: 3, val_acc:0.7612\n",
            "Accuracy improved from 0.7535 to 0.7612, saving model\n",
            "Iteration 13/180, loss:0.5061, acc:0.78202022-01-24 15:47:05.109932: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 131/180, loss:0.4944, acc:0.79402022-01-24 15:47:19.139123: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 168/180, loss:0.5029, acc:0.79202022-01-24 15:47:23.534473: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4962, acc:0.7780\n",
            "Epoch: 4, val_acc:0.7684\n",
            "Accuracy improved from 0.7612 to 0.7684, saving model\n",
            "Iteration 5/180, loss:0.5013, acc:0.76402022-01-24 15:47:27.900027: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 35/180, loss:0.4763, acc:0.79002022-01-24 15:47:31.456525: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 58/180, loss:0.4991, acc:0.77002022-01-24 15:47:34.179462: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 82/180, loss:0.4765, acc:0.79402022-01-24 15:47:37.056031: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 156/180, loss:0.5008, acc:0.77402022-01-24 15:47:45.886392: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4870, acc:0.7880\n",
            "Epoch: 5, val_acc:0.7707\n",
            "Accuracy improved from 0.7684 to 0.7707, saving model\n",
            "Iteration 1/180, loss:0.4802, acc:0.80202022-01-24 15:47:51.154727: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 60/180, loss:0.4855, acc:0.79202022-01-24 15:47:58.105003: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 113/180, loss:0.4762, acc:0.78602022-01-24 15:48:04.423004: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 135/180, loss:0.4741, acc:0.79802022-01-24 15:48:06.998611: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4546, acc:0.82602022-01-24 15:48:14.359009: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 6, val_acc:0.7748\n",
            "Accuracy improved from 0.7707 to 0.7748, saving model\n",
            "Iteration 28/180, loss:0.4446, acc:0.80802022-01-24 15:48:17.971237: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 67/180, loss:0.4653, acc:0.77802022-01-24 15:48:22.598801: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 134/180, loss:0.4534, acc:0.82002022-01-24 15:48:30.593469: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4491, acc:0.8000\n",
            "Epoch: 7, val_acc:0.7756\n",
            "Accuracy improved from 0.7748 to 0.7756, saving model\n",
            "Iteration 27/180, loss:0.4360, acc:0.80602022-01-24 15:48:41.767005: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 41/180, loss:0.4142, acc:0.84202022-01-24 15:48:43.404004: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 64/180, loss:0.4348, acc:0.81602022-01-24 15:48:46.149006: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 174/180, loss:0.4343, acc:0.81202022-01-24 15:48:59.291668: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4540, acc:0.80202022-01-24 15:49:00.007997: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-24 15:49:00.711010: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 8, val_acc:0.7787\n",
            "Accuracy improved from 0.7756 to 0.7787, saving model\n",
            "Iteration 11/180, loss:0.4558, acc:0.81202022-01-24 15:49:03.678864: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 132/180, loss:0.4444, acc:0.80602022-01-24 15:49:17.923418: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 177/180, loss:0.4657, acc:0.79602022-01-24 15:49:23.142552: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 179/180, loss:0.4361, acc:0.80602022-01-24 15:49:23.375072: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4385, acc:0.80402022-01-24 15:49:25.577006: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 9, val_acc:0.7800\n",
            "Accuracy improved from 0.7787 to 0.7800, saving model\n",
            "Iteration 8/180, loss:0.4345, acc:0.81002022-01-24 15:49:26.682973: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 39/180, loss:0.4425, acc:0.80802022-01-24 15:49:30.295966: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 173/180, loss:0.4548, acc:0.79802022-01-24 15:49:45.819294: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4363, acc:0.8120\n",
            "Epoch: 10, val_acc:0.7819\n",
            "Accuracy improved from 0.7800 to 0.7819, saving model\n",
            "Iteration 70/180, loss:0.4361, acc:0.81802022-01-24 15:49:56.969795: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4205, acc:0.83802022-01-24 15:50:10.055020: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 11, val_acc:0.7824\n",
            "Accuracy improved from 0.7819 to 0.7824, saving model\n",
            "Iteration 13/180, loss:0.4259, acc:0.81402022-01-24 15:50:13.514074: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 118/180, loss:0.4543, acc:0.80402022-01-24 15:50:25.798113: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4212, acc:0.82402022-01-24 15:50:33.644010: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-24 15:50:34.178993: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-24 15:50:34.522913: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-24 15:50:34.746009: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 12, val_acc:0.7833\n",
            "Accuracy improved from 0.7824 to 0.7833, saving model\n",
            "Iteration 42/180, loss:0.4034, acc:0.82602022-01-24 15:50:40.125644: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 44/180, loss:0.4011, acc:0.83602022-01-24 15:50:40.346137: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 89/180, loss:0.3909, acc:0.82602022-01-24 15:50:45.495628: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 113/180, loss:0.4131, acc:0.82202022-01-24 15:50:48.219592: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 114/180, loss:0.4529, acc:0.79802022-01-24 15:50:48.329589: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 124/180, loss:0.3854, acc:0.85402022-01-24 15:50:49.469666: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4072, acc:0.83602022-01-24 15:50:56.044001: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 13, val_acc:0.7849\n",
            "Accuracy improved from 0.7833 to 0.7849, saving model\n",
            "Iteration 42/180, loss:0.4484, acc:0.80402022-01-24 15:51:03.017229: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 43/180, loss:0.3952, acc:0.82602022-01-24 15:51:03.126933: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 113/180, loss:0.3825, acc:0.84802022-01-24 15:51:11.226988: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 120/180, loss:0.4150, acc:0.82602022-01-24 15:51:12.021112: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 133/180, loss:0.4211, acc:0.81202022-01-24 15:51:13.501734: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 142/180, loss:0.4174, acc:0.81402022-01-24 15:51:14.540297: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 152/180, loss:0.4325, acc:0.82602022-01-24 15:51:15.687643: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 161/180, loss:0.4188, acc:0.81602022-01-24 15:51:16.710779: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 177/180, loss:0.4226, acc:0.81402022-01-24 15:51:18.562930: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4258, acc:0.8120\n",
            "Epoch: 14, val_acc:0.7840\n",
            "Iteration 9/180, loss:0.4188, acc:0.83602022-01-24 15:51:22.155048: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 54/180, loss:0.4119, acc:0.81002022-01-24 15:51:27.344801: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 97/180, loss:0.4132, acc:0.82602022-01-24 15:51:32.267077: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 108/180, loss:0.4001, acc:0.82002022-01-24 15:51:33.526249: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 179/180, loss:0.3887, acc:0.84602022-01-24 15:51:41.695804: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4453, acc:0.79202022-01-24 15:51:42.790911: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 15, val_acc:0.7846\n",
            "Iteration 73/180, loss:0.4165, acc:0.81802022-01-24 15:51:52.486035: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 83/180, loss:0.3971, acc:0.82602022-01-24 15:51:53.634009: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 95/180, loss:0.4289, acc:0.81402022-01-24 15:51:55.039532: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3785, acc:0.8540\n",
            "Epoch: 16, val_acc:0.7847\n",
            "Iteration 84/180, loss:0.3968, acc:0.82202022-01-24 15:52:16.729118: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 91/180, loss:0.4090, acc:0.82002022-01-24 15:52:17.520202: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 151/180, loss:0.3997, acc:0.81602022-01-24 15:52:24.415073: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 159/180, loss:0.3841, acc:0.84602022-01-24 15:52:25.345014: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 171/180, loss:0.4078, acc:0.83802022-01-24 15:52:26.721193: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3950, acc:0.85002022-01-24 15:52:28.718126: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 17, val_acc:0.7834\n",
            "Iteration 114/180, loss:0.4049, acc:0.84002022-01-24 15:52:43.040975: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 117/180, loss:0.4145, acc:0.84802022-01-24 15:52:43.382158: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4073, acc:0.81002022-01-24 15:52:52.413712: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 18, val_acc:0.7850\n",
            "Accuracy improved from 0.7849 to 0.7850, saving model\n",
            "Iteration 7/180, loss:0.4106, acc:0.81802022-01-24 15:52:53.630525: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 13/180, loss:0.4143, acc:0.82002022-01-24 15:52:54.312176: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 29/180, loss:0.3877, acc:0.84602022-01-24 15:52:56.146486: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 49/180, loss:0.3773, acc:0.83802022-01-24 15:52:58.484730: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 93/180, loss:0.4165, acc:0.81602022-01-24 15:53:03.589791: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4226, acc:0.80802022-01-24 15:53:14.794003: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 19, val_acc:0.7839\n",
            "Iteration 14/180, loss:0.3789, acc:0.83402022-01-24 15:53:17.464868: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 26/180, loss:0.4285, acc:0.81402022-01-24 15:53:18.851731: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 121/180, loss:0.3946, acc:0.83202022-01-24 15:53:29.773371: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3698, acc:0.84002022-01-24 15:53:38.498991: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 20, val_acc:0.7843\n",
            "Iteration 30/180, loss:0.4091, acc:0.82202022-01-24 15:53:42.224773: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 59/180, loss:0.3692, acc:0.84002022-01-24 15:53:45.575268: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 64/180, loss:0.3918, acc:0.86202022-01-24 15:53:46.152758: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 170/180, loss:0.3836, acc:0.83002022-01-24 15:53:58.274790: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4035, acc:0.83202022-01-24 15:54:01.135002: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 21, val_acc:0.7841\n",
            "Iteration 65/180, loss:0.3803, acc:0.84202022-01-24 15:54:09.072998: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 104/180, loss:0.3706, acc:0.85402022-01-24 15:54:13.559997: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 147/180, loss:0.4024, acc:0.85202022-01-24 15:54:18.531591: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3717, acc:0.8340\n",
            "Epoch: 22, val_acc:0.7838\n",
            "Iteration 100/180, loss:0.3632, acc:0.85802022-01-24 15:54:36.104068: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 131/180, loss:0.3738, acc:0.84402022-01-24 15:54:39.633159: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3576, acc:0.8500\n",
            "Epoch: 23, val_acc:0.7833\n",
            "Iteration 50/180, loss:0.3636, acc:0.84202022-01-24 15:54:53.217151: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 108/180, loss:0.3375, acc:0.88202022-01-24 15:54:59.886125: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 139/180, loss:0.3645, acc:0.84602022-01-24 15:55:03.421003: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 159/180, loss:0.3787, acc:0.83002022-01-24 15:55:05.726638: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 168/180, loss:0.3639, acc:0.86602022-01-24 15:55:06.745783: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 169/180, loss:0.3713, acc:0.85002022-01-24 15:55:06.853294: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4101, acc:0.81802022-01-24 15:55:09.514106: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 24, val_acc:0.7830\n",
            "Iteration 76/180, loss:0.3380, acc:0.86802022-01-24 15:55:19.095969: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3546, acc:0.86202022-01-24 15:55:31.015559: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-24 15:55:31.778019: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 25, val_acc:0.7832\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to logistic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jqC0UUCwvnq",
        "outputId": "f4d7d76b-3c95-4b82-d9cc-3e3c2f85280b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6803/10000 = 68.0300 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIItdYFmw489",
        "outputId": "2efb3ec0-9655-4930-ffd5-83c1cdeb1bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7130/10000 = 71.3000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TtBW1i65z5yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = 'dataset/train-processed-processed.csv'\n",
        "TEST_PROCESSED_FILE = 'dataset/test-processed-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 1500\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 100\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            utils.write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(20)\n",
        "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = utils.top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = utils.split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=20)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        utils.write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            utils.write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            utils.write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        utils.save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NA3dTDNw8m2",
        "outputId": "9d797722-7495-4f3f-a1e2-305bb73c6ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7208/10000 = 72.0800 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/svm.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPwn6pGBxFql",
        "outputId": "ef0f58a4-73a9-477b-be32-42a711a4b7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7790/10000 = 77.9000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/neuralnet.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOgF4xEJxHt_",
        "outputId": "237b08b5-cfa2-4153-96ac-2bc8b69dd443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-24 16:04:24.076602: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Iteration 180/180, loss:0.5115, acc:0.7540\n",
            "Epoch: 1, val_acc:0.7575\n",
            "Accuracy improved from 0.0000 to 0.7575, saving model\n",
            "Iteration 180/180, loss:0.4494, acc:0.8020\n",
            "Epoch: 2, val_acc:0.7713\n",
            "Accuracy improved from 0.7575 to 0.7713, saving model\n",
            "Iteration 180/180, loss:0.4337, acc:0.7980\n",
            "Epoch: 3, val_acc:0.7721\n",
            "Accuracy improved from 0.7713 to 0.7721, saving model\n",
            "Iteration 180/180, loss:0.3862, acc:0.8320\n",
            "Epoch: 4, val_acc:0.7687\n",
            "Iteration 180/180, loss:0.4340, acc:0.81402022-01-24 16:05:44.005049: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 5, val_acc:0.7654\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OihYMHUxJoz",
        "outputId": "070629f2-6624-4f02-986c-3aefda6ee544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-24 16:06:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-24 16:06:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-24 16:06:21--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.18MB/s    in 2m 41s  \n",
            "\n",
            "2022-01-24 16:09:02 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZd1NofvxMcg",
        "outputId": "7c6f3596-13e1-4535-ad71-e4ce471c52d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w72NMVfUxOVt",
        "outputId": "3e00066d-7834-4526-906e-486b5f16503f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 400000/0\n",
            "\n",
            "Found 31735 words in GLOVE\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-24 16:15:02.120384: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 200)           18000200  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 200)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               168448    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,176,969\n",
            "Trainable params: 18,176,969\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "704/704 [==============================] - 16s 16ms/step - loss: 0.5377 - accuracy: 0.7302 - val_loss: 0.4769 - val_accuracy: 0.7741\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.4580 - accuracy: 0.7856 - val_loss: 0.4611 - val_accuracy: 0.7804\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.4109 - accuracy: 0.8157 - val_loss: 0.4629 - val_accuracy: 0.7770\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.3663 - accuracy: 0.8387 - val_loss: 0.5196 - val_accuracy: 0.7676\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.3279 - accuracy: 0.8570 - val_loss: 0.5001 - val_accuracy: 0.7744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/cnn.py"
      ],
      "metadata": {
        "id": "Ya2HEptmxQHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e67052-8bee-4fff-84ad-608b1fc40014"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE seeds\n",
            "Processing 400000/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-24 16:24:46.123406: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Epoch 1/8\n",
            "704/704 [==============================] - 27s 26ms/step - loss: 0.5305 - accuracy: 0.7296 - val_loss: 0.4812 - val_accuracy: 0.7599\n",
            "Epoch 2/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.4491 - accuracy: 0.7887 - val_loss: 0.4529 - val_accuracy: 0.7875\n",
            "Epoch 3/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.4002 - accuracy: 0.8187 - val_loss: 0.4636 - val_accuracy: 0.7796\n",
            "Epoch 4/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.3507 - accuracy: 0.8450 - val_loss: 0.4728 - val_accuracy: 0.7823\n",
            "Epoch 5/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.3104 - accuracy: 0.8648 - val_loss: 0.5057 - val_accuracy: 0.7706\n",
            "Epoch 6/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.2764 - accuracy: 0.8793 - val_loss: 0.5649 - val_accuracy: 0.7685\n",
            "Epoch 7/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.2458 - accuracy: 0.8939 - val_loss: 0.5786 - val_accuracy: 0.7679\n",
            "Epoch 8/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.2232 - accuracy: 0.9048 - val_loss: 0.6471 - val_accuracy: 0.7587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Oe6CiP4dxRzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}